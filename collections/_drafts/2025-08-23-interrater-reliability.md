---
layout: post
title: "Grad School Scramble III: Inter-rater Reliability"
date: 2025-08-23
summary: Part 3 of a multi-part series dedicated to investigating quant topics in preparation for grad school.
categories: quantitative statistics school academics
---

[comoputer pic]

Well, here we are! Summer is over, the new students are flocking to campus, and I find myself in the beautiful and hot state of Arizona. Classes technically started on Thursday, but they mostly consisted of looking over syllabi and introducing yourself to your classmates. The ASU program is great so far: my fellow grad students are awesome, the faculty are trememdous, and I've already begun immersing myself in local communities like choir and card shops.

During one of my Friday classes, Professional Issues in Psychology (or, as it's informally known, Intro to Being a Grad Student), the instructor Cheryl Conrad told an anecdotal story about inter-rater reliability. As she was telling it, I realized that I really don't know much about the concept, and decided that it would be a good installment of Grad School Scramble. I have not been nearly as prolific with this series as I had hoped, but I wanted to finish one last post before school gets heavily underway. Who knowsâ€”maybe I'll continue it into the semester.

### Inter-rater reliability: Definitions and concepts

Psychological studies and tests can often require responses that are long, involved, or otherwise unable to be directly assessed numerically. When such responses are present in a test, they are often interpreted by an individual (or individuals) to convert them to a format that can be analyzed. For example, essays on AP exams are sent to independent readers that rate the essays on a scale from 0-6. In AP's case, two independent readers will score each essay, so that an individual score isn't biased by one single reader. For more general tests and studies, these readers are generally known as "raters," and it's critically important that they are accurately and reliably assessing each response.

How can we determine if raters are doing a good job? One method is by evaluating the *inter-rater reliability*: a measure of how much the raters are agreeing with each other. If they display a large amount of agreement, we have reason to believe that their judgments are valid; conversely, if they disagree frequently, there might be reason to doubt the validity of their ratings. With that said, how do we measure their agreement?

#### 
